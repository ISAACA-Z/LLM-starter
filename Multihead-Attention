from torch import nn
import torch
import math

X = torch.randn(128, 64, 512)
#生成一个三维张量，从标准正态分布中随机取值。128为batch_size ，64为序列长度length l，512为embedding维度大小 d。

d_model = 512 #即embedding空间大小，或者叫dimension
n_head = 8  #8个头，d’ = 512/8

class multi_head_attention(nn.Module):
    def __init__(self, d_model, n_head ) -> None:#表示该函数不会返回任何值
        super().__init__()

        assert d_model % n_head == 0#保证embedding空间可以平均分配到各个注意力的头上

        self.n_head = n_head
        self.d_model = d_model
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_combine = nn.Linear(d_model, d_model)
        self.softmax = nn.Softmax(dim = -1)
        #dim = 0 对张量的行， 1 是列， -1 是最后一维

    def forward(self, q, k, v):
        batch, time, dimension = q.shape
        n_d = self.d_model//self.n_head#//表示整除，而n_d则是平均分到每个头上的dimension有多少
        q, k, v =self.w_q(q), self.w_k(k), self.w_v(v)

        q = q.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)
        #view函数用于调整张量形状，permute是排列维度顺序，现在的顺序是batch，self.n_head, time, n_d（time其实就是seq_len）
        #所以这里先将dimension拆成self.n_head, n_d两项，类似于一个头的分配
        #然后调整计算顺序
        k = k.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)
        v = v.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)

        score = q @ k.transpose(2, 3) / math.sqrt(n_d)
        #transpose效果与permute差不多，但是transpose只改变参数中的两个维度
        #这里即将batch，self.n_head, time, n_d变成batch，self.n_head, n_d, time

        mask = torch.tril(torch.ones(time, time, dtype = bool))
        #torch.tril:生成一个下三角矩阵， torch.ones可以生成一个 time*time维度的，内容全部填充为1的正方形矩阵
        #所以mask是一个下三角全为1，上三角全为0的矩阵，  这也是利用了mask_fill函数将“True”或者“1”的位置不处理，而“False”或“0”的位置进行赋值操作
        score = score.masked_fill(mask == 0, float("-inf"))
        #0的地方填充为负无穷“-inf”

        score = self.softmax(score)@v
        #先softmax再乘

        score = score.permute(0, 2, 1, 3).contiguous().view(batch, time, dimension)
        #顺序交换回来，并使之在内存上连续

        output = self.w_combine(score)
        return output

attention = multi_head_attention(d_model, n_head)

output = attention(X, X, X)
print(output, output.shape)





